#!/usr/bin/env python

import glob
import cPickle
import os
import multiprocessing
import copy
import re
import math
import time
from subprocess import Popen, PIPE, call
import matplotlib.pyplot as plt
from sequence_record import TCSeqRec
from distance_matrix import DistanceMatrix
from clustering import Clustering
from partition import Partition

# from simulation import Simulation

import copy_reg
import types
from random import shuffle
import shutil


def _pickle_method(method):
    """
    Adjust pickling via copy_reg module to make multiprocessing.Pool work
    with class methods (otherwise unpickleable)
    """

    func_name = method.im_func.__name__
    obj = method.im_self
    cls = method.im_class
    if func_name.startswith('__') and not func_name.endswith('__'):

        # deal with mangled names

        cls_name = cls.__name__.lstrip('_')
        func_name = '_%s%s' % (cls_name, func_name)
    return (_unpickle_method, (func_name, obj, cls))


def _unpickle_method(func_name, obj, cls):
    """
    Adjust pickling via copy_reg module to make multiprocessing.Pool work
    with class methods (otherwise unpickleable)
    """

    if obj and func_name in obj.__dict__:
        (cls, obj) = (obj, None)  # if func_name is classmethod
    for cls in cls.__mro__:
        try:
            func = cls.__dict__[func_name]
        except KeyError:
            pass
        else:
            break
    return func.__get__(obj, cls)


copy_reg.pickle(types.MethodType, _pickle_method, _unpickle_method)


class SequenceCollection(object):

    """
    Orchestrating class that should:
    a) work as a central repository for the information generated by the 
       subordinate classes, and
    b) be the only class directly interacted with by the user 

    TO DO:
    implement consistent naming of methods (where appropriate)
    Prefixes:
    get_[something]  - returns the object implied by something
    put_[something]  - puts something in the class data structure
    show_[something] - prints something to screen
    plot_[something] - displays a plot of something
    _[something]     - private method
    """

    # Store some mappings for data retrieval

    records_to_keys = {}
    keys_to_records = {}
    clusters_to_partitions = {}
    partitions = {}
    distance_matrices = {}
    concats = {}
    inferred_trees = {}

    # Store some data

    files = None
    records = []
    length = 0

    def __init__(
        self,
        input_dir=None,
        records=None,
        file_format='fasta',
        datatype='protein',
        helper='./class_files/DV_wrapper.drw',
        gtp_path='./class_files',
        tmpdir='/tmp',
        get_distances=True,
        parallel_load=True,
        overwrite=True,
        ):

        self.records = records
        self.gtp_path = gtp_path
        self.tmpdir = tmpdir

        # Lambda for sorting by name and number

        sort_key = lambda item: tuple((int(num) if num else alpha)
                for (num, alpha) in re.findall(r'(\d+)|(\D+)', item))

        # Can give an input directory as optional argument
        # If given:
        #    read the alignment files
        #    optionally calculate pairwise distances
        #    store the sequence data

        if input_dir:

            files = self.get_files(input_dir, file_format)

            # file checks

            if files == 0:
                print 'There was a problem reading files from {0}'.format(input_dir)
                return

            if get_distances and not os.path.isfile(helper):
                print 'There was a problem finding the darwin helper at {0}'.format(helper)
                return

            # done

            files.sort(key=sort_key)
            self.put_records(files, file_format, datatype)
            self.length = len(self.records)
            self.sanitise_records()
            if not os.path.isdir(tmpdir):
                os.mkdir(tmpdir)
            if get_distances:
                if parallel_load:
                    self.put_dv_matrices_parallel(helper=helper,
                            tmpdir=tmpdir, overwrite=overwrite)
                else:
                    self.put_dv_matrices(helper=helper, tmpdir=tmpdir,
                            overwrite=overwrite)
        elif records:

        # Can optionally give record objects directly if no input dir specified

            self.sanitise_records()
            if get_distances:
                if parallel_load:
                    self.put_dv_matrices_parallel(helper=helper,
                            tmpdir=tmpdir, overwrite=overwrite)
                else:
                    self.put_dv_matrices(helper=helper, tmpdir=tmpdir,
                            overwrite=overwrite)

            self.length = len(self.records)

    def __str__(self):
        s = 'SequenceCollection object:\n'
        s += 'Contains {0} alignments\n'.format(self.length)
        return s

    def get_files(self, input_dir, file_format='fasta'):
        """
        Get list of alignment files from an input directory
        *.fa, *.fas and *.phy files only
        Stores in self.files
        """

        if file_format == 'fasta':
            files = glob.glob('{0}/*.fa'.format(input_dir))
            if len(files) == 0:
                files = glob.glob('{0}/*.fas'.format(input_dir))
        elif file_format == 'phylip':
            files = glob.glob('{0}/*.phy'.format(input_dir))
        else:
            print 'Unrecognised file format %s' % file_format
            files = None
        if not files:
            print 'No sequence files found in {0}'.format(input_dir)
            return 0
        return sorted(files)

    def put_records(
        self,
        files,
        file_format='fasta',
        datatype='protein',
        ):
        """ 
        Reads sequence files from the list generated by
        get_files and stores in self.records
        """

        get_name = lambda i: i[i.rindex('/') + 1:i.rindex('.')]

        record_list = [TCSeqRec(f, file_format=file_format,
                       name=get_name(f), datatype=datatype) for f in
                       files]

        enumeration = enumerate(record_list, start=1)
        records_to_keys = dict([(name, number) for (number, name) in
                               enumerate(record_list)])
        keys_to_records = dict(enumerate(record_list))
        self.records = record_list
        self.records_to_keys = records_to_keys
        self.keys_to_records = keys_to_records

    def get_records(self):
        """
        Returns list of stored sequence records
        """

        return [self.keys_to_records[i] for i in range(self.length)]

    def sanitise_records(self):
        for rec in self.records:
            rec.sanitise()

    def put_dv_matrices(
        self,
        tmpdir='/tmp',
        helper='./class_files/DV_wrapper.drw',
        overwrite=True,
        ):

        for rec in self.records:
            rec.dv = [rec.get_dv_matrix(tmpdir=tmpdir, helper=helper,
                      overwrite=overwrite)]

    def _unpack_dv(self, packed_args):
        return packed_args[0].get_dv_matrix(*packed_args[1:])

    def _dv_parallel_call(
        self,
        tmpdir='/tmp',
        helper='./class_files/DV_wrapper.drw',
        overwrite=True,
        ):

        nprocesses = min(len(self.records), multiprocessing.cpu_count()
                         - 1)
        print 'Initialising a pool of {0} processes running {1} jobs...'.format(nprocesses,
                len(self.records))
        pool = multiprocessing.Pool(nprocesses)
        results = []
        args = []
        names = []
        for rec in self.records:
            new_dir = tmpdir + '/' + rec.name
            if not os.path.isdir(new_dir):
                os.mkdir(new_dir)
            args.append((rec, tmpdir + '/' + rec.name, helper,
                        overwrite))
            names.append(rec.name)
        r = pool.map_async(self._unpack_dv, args,
                           callback=results.append)
        r.wait()
        for (w, x, y, z) in args:
            if os.path.isdir(x):
                os.rmdir(x)
        results = results[0]
        print 'Results obtained, closing pool...'
        pool.close()
        pool.join()
        print 'Pool closed'
        return dict(zip(names, results))

    def put_dv_matrices_parallel(
        self,
        tmpdir='/tmp',
        helper='./class_files/DV_wrapper.drw',
        overwrite=True,
        ):

        dv_matrices_dict = self._dv_parallel_call(tmpdir, helper,
                overwrite=overwrite)
        for rec in self.records:
            rec.dv = [dv_matrices_dict[rec.name]]

    def get_dv_matrices(self):
        dvs = {}
        for rec in self.get_records():
            dvs[rec.name] = rec.dv
        return dvs

    def _unpack_bionj(self, packed_args):
        return packed_args[0].get_bionj_tree(*packed_args[1:])

    def _bionj_parallel_call(
        self,
        model=None,
        datatype=None,
        rec_list=None,
        ncat=1,
        tmpdir='/tmp',
        overwrite=True,
        ):

        if not rec_list:
            rec_list = self.records
        nprocesses = min(len(rec_list), multiprocessing.cpu_count() - 1)
        print 'Initialising a pool of {0} processes running {1} jobs...'.format(nprocesses,
                len(rec_list))
        pool = multiprocessing.Pool(nprocesses)
        results = []
        args = []
        names = []
        for rec in rec_list:
            args.append((
                rec,
                model,
                datatype,
                ncat,
                tmpdir,
                overwrite,
                ))
            names.append(rec.name)
        r = pool.map_async(self._unpack_bionj, args,
                           callback=results.append)
        r.wait()
        print 'Results obtained, closing pool...'
        pool.close()
        pool.join()
        print 'Pool closed'
        return dict(zip(names, results[0]))

    def _unpack_phyml(self, packed_args):
        return packed_args[0].get_phyml_tree(*packed_args[1:])

    def _phyml_parallel_call(
        self,
        model=None,
        datatype=None,
        rec_list=None,
        ncat=4,
        tmpdir='/tmp',
        overwrite=True,
        ):

        if not rec_list:
            rec_list = self.records
        nprocesses = min(len(rec_list), multiprocessing.cpu_count() - 1)
        print 'Initialising a pool of {0} processes running {1} jobs...'.format(nprocesses,
                len(rec_list))
        pool = multiprocessing.Pool(nprocesses)
        results = []
        args = []
        names = []
        for rec in rec_list:
            args.append((
                rec,
                model,
                datatype,
                ncat,
                tmpdir,
                overwrite,
                ))
            names.append(rec.name)
        r = pool.map_async(self._unpack_phyml, args,
                           callback=results.append)
        r.wait()
        print 'Results obtained, closing pool...'
        pool.close()
        pool.join()
        print 'Pool closed'
        return dict(zip(names, results[0]))

    def _unpack_raxml(self, packed_args):
        return packed_args[0].get_raxml_tree(*packed_args[1:])

    def _raxml_parallel_call(
        self,
        rec_list=None,
        tmpdir='/tmp',
        overwrite=True,
        ):

        if not rec_list:
            rec_list = self.records
        nprocesses = multiprocessing.cpu_count() - 1
        print 'Initialising a pool of {0} processes running {1} jobs...'.format(nprocesses,
                len(rec_list))
        pool = multiprocessing.Pool(nprocesses)
        results = []
        args = []
        names = []
        for rec in rec_list:
            args.append((rec, tmpdir, overwrite))
            names.append(rec.name)
        r = pool.map_async(self._unpack_raxml, args,
                           callback=results.append)
        r.wait()
        pool.close()
        pool.join()
        return dict(zip(names, results[0]))

    def _unpack_TC(self, packed_args):
        return packed_args[0].get_TC_tree(*packed_args[1:])

    def _TC_parallel_call(
        self,
        rec_list=None,
        tmpdir='/tmp',
        overwrite=True,
        ):

        if not rec_list:
            rec_list = self.records
        nprocesses = multiprocessing.cpu_count() - 1
        print 'Initialising a pool of {0} processes running {1} jobs...'.format(nprocesses,
                len(rec_list))
        pool = multiprocessing.Pool(nprocesses)
        results = []
        args = []
        names = []
        for rec in rec_list:
            args.append((rec, tmpdir, overwrite))
            names.append(rec.name)
        r = pool.map_async(self._unpack_TC, args,
                           callback=results.append)
        r.wait()
        pool.close()
        pool.join()
        return dict(zip(names, results[0]))

    def put_trees(
        self,
        rec_list=None,
        program='treecollection',
        model=None,
        datatype=None,
        ncat=4,
        tmpdir=None,
        overwrite=True,
        ):
        if tmpdir is None:
            tmpdir=self.tmpdir
        if not program in ['treecollection', 'raxml', 'phyml', 'bionj']:
            print 'unrecognised program {0}'.format(program)
            return
        if not rec_list:
            rec_list = self.records
        for rec in rec_list:
            if overwrite is False:
                if rec.name in self.inferred_trees:
                    continue
            if program == 'treecollection':
                tree = rec.get_TC_tree(tmpdir=tmpdir,
                        overwrite=overwrite)
            elif program == 'raxml':
                tree = rec.get_raxml_tree(tmpdir=tmpdir,
                        overwrite=overwrite)
            elif program == 'phyml':
                tree = rec.get_phyml_tree(model=model,
                        datatype=datatype, tmpdir=tmpdir, ncat=ncat,
                        overwrite=overwrite)
            elif program == 'bionj':
                tree = rec.get_bionj_tree(model=model,
                        datatype=datatype, tmpdir=tmpdir, ncat=ncat,
                        overwrite=overwrite)
            self.inferred_trees[rec.name] = tree

    def put_trees_lsf(
        self,
        rec_list=None,
        program='phyml',
        model=None,
        datatype=None,
        ncat=4,
        lsf_tmpdir='./lsf_tmp',
        overwrite=True,
        basepath='.',
        ):

        def which_dir(n, dirs):
            """ 
            Given a number, n, and a set of directories of the form:
            /1-1000, /1001-2000, /2001-3000 ...
            returns the directory corresponding to the range the number
            fits into
            """

            for x in dirs:
                (lower, upper) = x[x.rindex('/') + 1:].split('-')
                if int(lower) <= n <= int(upper):
                    return x

        def is_bjob_done(id):

            cmd = 'bjobs {0}'.format(id)
            process = Popen(cmd, stdout=PIPE, shell=True)
            process.wait()
            output = process.communicate()[0].split('\n')[1]
            if 'DONE' in output:
                return True
            else:
                return False

        def extract_jobid(s):
            return int(re.findall('(?<=\<)[\d+]+(?=\>)', s)[0])

        getname = lambda x: x[x.rindex('/') + 1:x.rindex('.')]

        # Make paths into absolute paths, because these jobs will operate
        # on separate machines

        lsf_tmpdir = os.path.abspath(lsf_tmpdir)
        basepath = os.path.abspath(basepath)

        # Most cases will use self.records

        if not rec_list:
            rec_list = self.records

        # Create temp directory reachable by lsf machine

        if not os.path.isdir(lsf_tmpdir):
            os.mkdir(lsf_tmpdir)

        # Need a runner script to be called by lsf JobArray

        if program == 'phyml':
            runner_script = '{0}/lsf_phyml.py'.format(basepath)
        elif program == 'raxml':
            runner_script = '{0}/lsf_raxml.py'.format(basepath)
        elif program == 'bionj':
            runner_script = '{0}/lsf_bionj.py'.format(basepath)
        elif program == 'treecollection':
            runner_script = '{0}/lsf_treecollection.py'.format(basepath)
        else:
            print 'Program {0} not recognised'.format(program)
            return

        # Use tempwrapper script to create and delete tmp directories
        # on the lsf machines

        tempwrapper = '{0}/tempdir_wrapper.sh'.format(basepath)

        # Filechecks

        if not os.path.isfile(runner_script):
            print '{0}: file not found'.format(runner_script)
        if not os.path.isfile(tempwrapper):
            print '{0}: file not found'.format(tempwrapper)

        # Create batch directories for all jobs so that they run in
        # batches of no more than 1000 (max for JobArray)

        avail_dirs = []
        for i in range(int(math.floor(len(rec_list) / 1000) + 1)):
            dname = '{0}/{1}-{2}'.format(lsf_tmpdir, i * 1000 + 1, (i
                    + 1) * 1000)
            avail_dirs.append(dname)
            if not os.path.isdir(dname):
                os.mkdir(dname)

        # Write a phylip file into the lsf_temp directory,
        # and write a jobfile pointing to it, with indexed file extension

        for (i, rec) in enumerate(rec_list, start=1):
            filename = '{0}/{1}.phy'.format(lsf_tmpdir, rec.name)
            rec.write_phylip(filename)
            d = which_dir(i, avail_dirs)
            with open('{0}/job.{1}'.format(d, i), 'w') as file:
                file.write(filename)

        # Use a masterscript so that we wait for all jobs to complete before moving on

        commands = []
        for path in avail_dirs:
            numbers = path[path.rindex('/') + 1:]
            (lower, upper) = numbers.split('-')
            lower = int(lower)
            upper = min(i, int(upper))

            # the jobfile is selected by the indexed file extension, which
            # is accessed using the $LSF_JOBINDEX environmental variable
            # (accessed internally by the runner script)

            commands.append('bsub -o /dev/null -J "trees[{0}-{1}]" bash {2} python {3} --model={4} --ncat={5} --datatype={6} -f {7}/job.\n'.format(
                lower,
                upper,
                tempwrapper,
                runner_script,
                model,
                ncat,
                datatype,
                path,
                ))

        # Set the masterscript running and wait for the results - pickled tree objects

        print 'Running LSF commands...'
        processes = [Popen(command, shell=True, stdin=PIPE,
                     stdout=PIPE, stderr=PIPE) for command in commands]
        jobids = [extract_jobid(process.communicate()[0])
                  for process in processes]
        while not all(is_bjob_done(jobid) for jobid in jobids):
            time.sleep(2)

        pickles = dict((getname(x), x) for x in
                       glob.glob('{0}/*.pickle'.format(lsf_tmpdir)))

        for rec in rec_list:
            rec.tree = cPickle.load(file(pickles[rec.name]))

        shutil.rmtree(lsf_tmpdir)

    def put_trees_parallel(
        self,
        rec_list=None,
        program='treecollection',
        model=None,
        datatype=None,
        ncat=4,
        tmpdir='/tmp',
        overwrite=True,
        ):

        if not program in ['treecollection', 'raxml', 'phyml', 'bionj']:
            print 'unrecognised program {0}'.format(program)
            return
        if not rec_list:
            rec_list = self.records
        if program == 'treecollection':
            trees_dict = self._TC_parallel_call(rec_list=rec_list,
                    tmpdir=tmpdir, overwrite=overwrite)
        elif program == 'raxml':
            trees_dict = self._raxml_parallel_call(rec_list=rec_list,
                    tmpdir=tmpdir, overwrite=overwrite)
        elif program == 'phyml':
            trees_dict = self._phyml_parallel_call(
                rec_list=rec_list,
                model=model,
                datatype=datatype,
                tmpdir=tmpdir,
                ncat=ncat,
                overwrite=overwrite,
                )
        elif program == 'bionj':
            trees_dict = self._bionj_parallel_call(
                rec_list=rec_list,
                model=model,
                datatype=datatype,
                tmpdir=tmpdir,
                ncat=ncat,
                overwrite=overwrite,
                )
        for rec in self.records:
            rec.tree = trees_dict[rec.name]
            self.inferred_trees[rec.name] = trees_dict[rec.name]

    def get_trees(self):
        return [rec.tree for rec in self.get_records()]

    def put_distance_matrices(
        self,
        metrics,
        tmpdir='/tmp',
        gtp_path=None,
        normalise=False,
        ):
        """
        Pass this function a list of metrics
        valid kwargs - invert (bool), normalise (bool)
        """

        if not gtp_path:
            gtp_path = self.gtp_path
        if not isinstance(metrics, list):
            metrics = [metrics]
        trees = [rec.tree for rec in self.records]
        for metric in metrics:
            dm = DistanceMatrix(trees, tmpdir=tmpdir, gtp_path=gtp_path)
            dm.get_distance_matrix(metric, normalise=normalise)
            self.distance_matrices[metric] = dm

    def get_distance_matrices(self):
        return self.distance_matrices

    def put_partition(
        self,
        metric,
        cluster_method,
        nclusters,
        prune=True,
        tmpdir=None,
        gtp_path=None,
        ):

        if not tmpdir:
            tmpdir = self.tmpdir
        if not gtp_path:
            gtp_path = self.gtp_path
        if not metric in self.get_distance_matrices():
            self.put_distance_matrices(metric, tmpdir=tmpdir,
                    gtp_path=gtp_path)
        partition_vector = \
            Clustering().run_clustering(self.distance_matrices[metric],
                cluster_method, nclusters, prune=prune)

        self.clusters_to_partitions[(metric, cluster_method,
                                    nclusters)] = partition_vector
        self.partitions[partition_vector] = Partition(partition_vector)
        return partition_vector

    def put_partitions(
        self,
        metrics,
        cluster_methods,
        nclusters,
        prune=True,
        tmpdir=None,
        gtp_path=None,
        ):
        """
        metrics, linkages and nclasses are given as lists, or coerced into 
        lists
        """

        if not isinstance(metrics, list):
            metrics = [metrics]
        if not isinstance(cluster_methods, list):
            cluster_methods = [cluster_methods]
        if not isinstance(nclusters, list):
            nclusters = [nclusters]
        if tmpdir is None:
            tmpdir = self.tmpdir
        if gtp_path is None:
            gtp_path = self.gtp_path
        else:
            nclusters = sorted(nclusters, reverse=True)
        names = [rec.name for rec in self.records]
        for metric in metrics:
            for cluster_method in cluster_methods:
                for n in nclusters:
                    key = (metric, cluster_method, n)
                    if key in self.clusters_to_partitions:
                        continue
                    else:
                        self.put_partition(
                            metric,
                            cluster_method,
                            n,
                            prune=prune,
                            tmpdir=tmpdir,
                            gtp_path=gtp_path,
                            )

    def concatenate_records(self):
        for p in self.partitions.values():
            p.concatenate_records(self.keys_to_records)
            for concat in p.concats:
                if not concat.name in self.concats:
                    self.concats[concat.name] = concat

    def get_partitions(self):
        return self.clustering.partitions

    def put_clusters(self):
        for key in self.clustering.partitions:
            if key not in self.clustering.clusters:
                self.clustering.concatenate_records(key, self.records)

    def get_clusters(self):
        return self.clustering.clusters

    def get_cluster_records(self):
        """
        Returns all concatenated records from cluster analysis
        """
        print 'Inferring {0} cluster trees'.format(len(self.concats))
        sort_key = lambda item: tuple((int(num) if num else alpha)
                for (num, alpha) in re.findall(r'(\d+)|(\D+)', item.name))
        return sorted(self.concats.values(), key=sort_key)

    def put_cluster_trees(
        self,
        program='treecollection',
        model=None,
        datatype=None,
        ncat=4,
        tmpdir='/tmp',
        overwrite=True,
        ):

        if program not in ['treecollection', 'raxml', 'phyml', 'bionj']:
            print 'unrecognised program {0}'.format(program)
            return
        rec_list = self.get_cluster_records()
        self.put_trees(
            rec_list=rec_list,
            program=program,
            model=model,
            ncat=ncat,
            datatype=datatype,
            tmpdir=tmpdir,
            overwrite=overwrite,
            )
        self.update_results()

    def update_results(self):
        for partition in self.partitions.values():
            partition.update_score(self.concats)

    def put_cluster_trees_parallel(
        self,
        program='treecollection',
        model=None,
        datatype=None,
        ncat=4,
        tmpdir='/tmp',
        overwrite=True,
        ):

        if program not in ['treecollection', 'raxml', 'phyml', 'bionj']:
            print 'unrecognised program {0}'.format(program)
            return
        rec_list = self.get_cluster_records()
        if program == 'treecollection':
            cluster_trees_dict = \
                self._TC_parallel_call(rec_list=rec_list,
                    tmpdir=tmpdir, overwrite=overwrite)
        elif program == 'raxml':
            cluster_trees_dict = \
                self._raxml_parallel_call(rec_list=rec_list,
                    tmpdir=tmpdir, overwrite=overwrite)
        elif program == 'phyml':
            cluster_trees_dict = self._phyml_parallel_call(
                rec_list=rec_list,
                model=model,
                datatype=datatype,
                ncat=ncat,
                tmpdir=tmpdir,
                overwrite=overwrite,
                )
        elif program == 'bionj':
            cluster_trees_dict = self._bionj_parallel_call(
                rec_list=rec_list,
                model=model,
                datatype=datatype,
                ncat=ncat,
                tmpdir=tmpdir,
                overwrite=overwrite,
                )
        for rec in rec_list:
            rec.tree = cluster_trees_dict[rec.name]
        self.update_results()

    def get_cluster_trees(self):
        rec_list = sorted(self.get_cluster_records(), key=lambda rec: \
                          rec.name)
        trees = [rec.tree for rec in rec_list]
        return trees

    def get_randomised_alignments(self):

        def pivot(lst):
            new_lst = zip(*lst)
            return [''.join(x) for x in new_lst]

        lengths = [rec.seqlength for rec in self.records]
        datatype = self.records[0].datatype
        concat = copy.deepcopy(self.records[0])
        for rec in self.records[1:]:
            concat += rec
        columns = pivot(concat.sequences)
        shuffle(columns)
        newcols = []
        for l in lengths:
            newcols.append(columns[:l])
            columns = columns[l:]
        newrecs = []
        for col in newcols:
            newseqs = pivot(col)
            newrec = TCSeqRec(headers=concat.headers,
                              sequences=newseqs, datatype=datatype)
            newrecs.append(newrec)
        for i in range(self.length):
            newrecs[i].name = self.records[i].name
        return newrecs

    def make_randomised_copy(self, program='treecollection',
                             tmpdir='/tmp'):

        other = copy.deepcopy(self)
        other.records = self.get_randomised_alignments()
        other.put_dv_matrices_parallel()
        other.put_trees_parallel(program=program, tmpdir=tmpdir)
        if self.get_clusters():
            other.clustering.clusters = {}
            other.put_clusters()
        if self.get_cluster_trees()[0].newick:
            cluster_program = \
                self.get_cluster_trees()[0].program.lower()
            other.put_cluster_trees_parallel(program=cluster_program,
                    tmpdir=tmpdir)
        return other

    def show_memberships(self):

        partitions = self.get_partitions()
        for compound_key in partitions:
            print ' '.join(str(x) for x in compound_key)
            partition = partitions[compound_key]
            print partition
            print self.clustering.get_memberships(partition)

    def plot_dendrogram(
        self,
        metric,
        link,
        nclasses,
        show=True,
        ):

        plot_object = self.clustering.plot_dendrogram((metric, link,
                nclasses))
        if show:
            plot_object.show()
        return plot_object

    def scree_plot(
        self,
        metric,
        link,
        *args
        ):

        fig = plt.figure()
        cl = self.get_clusters()
        (x, y) = ([], [])
        for k in sorted(cl):
            if metric in k and link in k:
                y.append(cl[k].score)
                x.append(k[-1])
        plt.plot(x, y, *args)
        return fig

    # def find_mergeable_groups(self, compound_key):

    #     result_object = self.get_clusters()[compound_key]
    #     cluster_trees = [rec.tree for rec in result_object.concats]
    #     cluster_names = [tree.name for tree in cluster_trees]
    #     matrix = self.clustering.get_distance_matrix(cluster_trees,
    #             'sym')
    #     groups = result_object.find_mergeable_groups(matrix)
    #     return groups

    # def merge_groups(self, compound_key):

    #     group_dict = self.find_mergeable_groups(compound_key)[0]
    #     old_memberships = self.get_clusters()[compound_key].members
    #     new_memberships = []
    #     skips = []
    #     for i in range(len(old_memberships)):
    #         if i in skips:
    #             continue
    #         elif i in group_dict:
    #             new = []
    #             new += old_memberships[i]
    #             for val in group_dict[i]:
    #                 new += old_memberships[val]
    #                 skips.append(val)
    #             new_memberships.append(new)
    #             continue
    #         new_memberships.append(old_memberships[i])

    #     new_partition = [0 for rec in self.records]
    #     i = 1
    #     for group in new_memberships:
    #         for member in group:
    #             index = self.records.index(member)
    #             new_partition[index] = i
    #         i += 1

    #     new_key = compound_key + ('merge', )
    #     self.clustering.partitions[new_key] = new_partition
    #     self.put_clusters()

    def simulate_from_result(
        self,
        compound_key,
        helper='./class_files/DV_wrapper.drw',
        tmpdir='/tmp',
        ):

        shorten = lambda x: '_'.join([str(b)[:5] for b in x])
        result_object = self.get_clusters()[compound_key]
        lengths = [[rec.seqlength for rec in m] for m in
                   result_object.members]
        total_lengths = [sum(x) for x in lengths]
        msa_dir = '{0}/msa'.format(tmpdir)
        if not os.path.isdir(msa_dir):
            os.mkdir(msa_dir)
        k = 1
        for i in range(result_object.length):
            tree = result_object.concats[i].tree
            tree = tree.pam2sps('sps2pam')
            treefile = \
                tree.write_to_file('{0}/{1}_tmptree{2}.nwk'.format(tmpdir,
                                   shorten(compound_key), i))
            outfile = '{0}/{1}_class{2}_params.drw'.format(tmpdir,
                    shorten(compound_key), i)
            length_list = lengths[i]
            total_length = (total_lengths[i] + total_lengths[i] % 3) / 3
            result_object.write_ALF_parameters(
                'alfsim_{0}'.format(i),
                tmpdir,
                'alftmp_{0}'.format(i),
                1,
                total_length,
                treefile,
                outfile,
                )
            os.system('alfsim {0}'.format(outfile))
            record = \
                TCSeqRec(glob.glob('{0}/alftmp_{1}/alfsim_{1}/MSA/*dna.fa'.format(tmpdir,
                         i))[0])

            alf_newick = \
                open('{0}/alftmp_{1}/alfsim_{1}/RealTree.nwk'.format(tmpdir,
                     i)).read()
            replacement_dict = dict(zip(re.findall(r'(\w+)(?=:)',
                                    alf_newick),
                                    re.findall(r'(\w+)(?=:)',
                                    tree.newick)))
            print alf_newick
            print tree.newick
            print replacement_dict
            record.sort_by_name()
            headers = [replacement_dict[x[:x.rindex('/')]] for x in
                       record.headers]
            print headers
            sequences = record.sequences
            print record
            for j in range(len(length_list)):
                start = sum(length_list[:j])
                end = sum(length_list[:j + 1])
                new_sequences = [seq[start:end] for seq in sequences]
                newmsa = TCSeqRec(headers=headers,
                                  sequences=new_sequences,
                                  name='gene{0:0>3}'.format(k))
                k += 1
                newmsa.write_fasta(outfile='{0}/{1}.fas'.format(msa_dir,
                                   newmsa.name))
            shutil.rmtree('{0}/alftmp_{1}'.format(tmpdir, i))
            os.remove(treefile)
            os.remove(outfile)

        new_seqcol_object = SequenceCollection(msa_dir, datatype='dna',
                helper=helper, tmpdir=tmpdir)
        shutil.rmtree('{0}/msa'.format(tmpdir))
        return new_seqcol_object
