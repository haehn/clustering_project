#!/usr/bin/python
# -*- coding: utf-8 -*-

import glob
import os
import multiprocessing
import copy
import matplotlib.pyplot as plt
from sequence_record import TCSeqRec
from clustering import Clustering
import copy_reg
import types
from random import shuffle


def _pickle_method(method):
    """
    Adjust pickling via copy_reg module to make multiprocessing.Pool work
    with class methods (otherwise unpickleable)
    """

    func_name = method.im_func.__name__
    obj = method.im_self
    cls = method.im_class
    if func_name.startswith('__') and not func_name.endswith('__'):

        # deal with mangled names

        cls_name = cls.__name__.lstrip('_')
        func_name = '_%s%s' % (cls_name, func_name)
    return (_unpickle_method, (func_name, obj, cls))


def _unpickle_method(func_name, obj, cls):
    """
    Adjust pickling via copy_reg module to make multiprocessing.Pool work
    with class methods (otherwise unpickleable)
    """

    if obj and func_name in obj.__dict__:
        (cls, obj) = (obj, None)  # if func_name is classmethod
    for cls in cls.__mro__:
        try:
            func = cls.__dict__[func_name]
        except KeyError:
            pass
        else:
            break
    return func.__get__(obj, cls)


copy_reg.pickle(types.MethodType, _pickle_method, _unpickle_method)


class SequenceCollection(object):

    """
    Orchestrating class that should:
    a) work as a central repository for the information generated by the 
       subordinate classes, and
    b) be the only class directly interacted with by the user 

    TO DO:
    implement consistent naming of methods (where appropriate)
    Prefixes:
    get_[something]  - returns the object implied by something
    put_[something]  - puts something in the class data structure
    show_[something] - prints something to screen
    plot_[something] - displays a plot of something
    _[something]     - private method
    """

    def get_files(self, input_dir, file_format='fasta'):
        """
        Get list of alignment files from an input directory
        *.fa, *.fas and *.phy files only
        Stores in self.files
        """

        if file_format == 'fasta':
            files = glob.glob('{0}/*.fa'.format(input_dir))
            if len(files) == 0:
                files = glob.glob('{0}/*.fas'.format(input_dir))
        elif file_format == 'phylip':
            files = glob.glob('{0}/*.phy'.format(input_dir))
        else:
            print 'Unrecognised file format %s' % file_format
            files = None
        if len(files) == 0:
            print 'No sequence files found in {0}'.format(input_dir)
            return
        return sorted(files)

    def put_records(
        self,
        files,
        file_format='fasta',
        datatype='protein',
        ):
        """ 
        Reads sequence files from the list generated by
        get_files and stores in self.records
        """

        name = lambda i: i[i.rindex('/') + 1:i.rindex('.')]
        self.records = [TCSeqRec(f, file_format=file_format,
                        name=name(f), datatype=datatype) for f in files]

    def get_records(self):
        """
        Returns list of stored sequence records
        """

        return self.records

    def sanitise_records(self):
        for rec in self.records:
            rec.sanitise()

    def put_dv_matrices(self, tmpdir='/tmp',
                        helper='./class_files/DV_wrapper.drw'):

        for rec in self.records:
            rec.dv = [rec.get_dv_matrix(tmpdir=tmpdir, helper=helper)]

    def _unpack_dv(self, packed_args):
        return packed_args[0].get_dv_matrix(packed_args[1],
                packed_args[2])

    def _dv_parallel_call(self, tmpdir='/tmp',
                          helper='./class_files/DV_wrapper.drw'):

        nprocesses = multiprocessing.cpu_count() - 1
        pool = multiprocessing.Pool(nprocesses)
        results = []
        args = []
        names = []
        for rec in self.records:
            new_dir = tmpdir + '/' + rec.name
            if not os.path.isdir(new_dir):
                os.mkdir(new_dir)
            args.append((rec, tmpdir + '/' + rec.name, helper))
            names.append(rec.name)
        r = pool.map_async(self._unpack_dv, args,
                           callback=results.append)
        r.wait()
        for (x, y, z) in args:
            if os.path.isdir(y):
                os.rmdir(y)
        results = results[0]
        pool.close()
        pool.join()
        return dict(zip(names, results))

    def put_dv_matrices_parallel(self, tmpdir='/tmp',
                                 helper='./class_files/DV_wrapper.drw'):

        dv_matrices_dict = self._dv_parallel_call(tmpdir, helper)
        for rec in self.records:
            rec.dv = [dv_matrices_dict[rec.name]]

    def get_dv_matrices(self):
        dvs = {}
        for rec in self.get_records():
            dvs[rec.name] = rec.dv
        return dvs

    def _unpack_phyml(self, packed_args):
        return packed_args[0].get_phyml_tree(packed_args[1])

    def _phyml_parallel_call(self, rec_list=None, tmpdir='/tmp'):
        if not rec_list:
            rec_list = self.records
        nprocesses = multiprocessing.cpu_count() - 1
        pool = multiprocessing.Pool(nprocesses)
        results = []
        args = []
        names = []
        for rec in rec_list:
            args.append((rec, tmpdir))
            names.append(rec.name)
        r = pool.map_async(self._unpack_phyml, args,
                           callback=results.append)
        r.wait()
        pool.close()
        pool.join()
        return dict(zip(names, results[0]))

    def _unpack_raxml(self, packed_args):
        return packed_args[0].get_raxml_tree(packed_args[1])

    def _raxml_parallel_call(self, rec_list=None, tmpdir='/tmp'):
        if not rec_list:
            rec_list = self.records
        nprocesses = multiprocessing.cpu_count() - 1
        pool = multiprocessing.Pool(nprocesses)
        results = []
        args = []
        names = []
        for rec in rec_list:
            args.append((rec, tmpdir))
            names.append(rec.name)
        r = pool.map_async(self._unpack_raxml, args,
                           callback=results.append)
        r.wait()
        pool.close()
        pool.join()
        return dict(zip(names, results[0]))

    def _unpack_TC(self, packed_args):
        return packed_args[0].get_TC_tree(packed_args[1])

    def _TC_parallel_call(self, rec_list=None, tmpdir='/tmp'):
        if not rec_list:
            rec_list = self.records
        nprocesses = multiprocessing.cpu_count() - 1
        pool = multiprocessing.Pool(nprocesses)
        results = []
        args = []
        names = []
        for rec in rec_list:
            args.append((rec, tmpdir))
            names.append(rec.name)
        r = pool.map_async(self._unpack_TC, args,
                           callback=results.append)
        r.wait()
        pool.close()
        pool.join()
        return dict(zip(names, results[0]))

    def put_trees(
        self,
        rec_list=None,
        program='treecollection',
        tmpdir='/tmp',
        ):

        if not program in ['treecollection', 'raxml', 'phyml']:
            print 'unrecognised program {0}'.format(program)
            return
        if not rec_list:
            rec_list = self.records
        for rec in rec_list:
            if program == 'treecollection':
                rec.get_TC_tree(tmpdir)
            elif program == 'raxml':
                rec.get_raxml_tree(tmpdir)
            elif program == 'phyml':
                rec.get_phyml_tree(tmpdir)

    def put_trees_parallel(
        self,
        rec_list=None,
        program='treecollection',
        tmpdir='/tmp',
        ):

        if not program in ['treecollection', 'raxml', 'phyml']:
            print 'unrecognised program {0}'.format(program)
            return
        if not rec_list:
            rec_list = self.records
        if program == 'treecollection':
            trees_dict = self._TC_parallel_call(rec_list=rec_list,
                    tmpdir=tmpdir)
        elif program == 'raxml':
            trees_dict = self._raxml_parallel_call(rec_list=rec_list,
                    tmpdir=tmpdir)
        elif program == 'phyml':
            trees_dict = self._phyml_parallel_call(rec_list=rec_list,
                    tmpdir=tmpdir)
        for rec in self.records:
            rec.tree = trees_dict[rec.name]

    def get_trees(self):
        trees = {}
        for rec in self.records:
            trees[rec.name] = rec.tree
        return trees

    def put_distance_matrices(self, metrics, **kwargs):
        """
        Pass this function a list of metrics
        valid kwargs - invert (bool), normalise (bool)
        """

        if not isinstance(metrics, list):
            metrics = [metrics]
        trees = [rec.tree for rec in self.records]
        for metric in metrics:
            self.clustering.put_distance_matrix(trees, metric, **kwargs)

    def get_distance_matrices(self):
        return self.clustering.distance_matrices

    def put_partitions(
        self,
        metrics,
        linkages,
        nclasses,
        criterion='distance',
        ):
        """
        metrics, linkages and nclasses are given as lists, or coerced into 
        lists
        """

        if not isinstance(metrics, list):
            metrics = [metrics]
        if not isinstance(linkages, list):
            linkages = [linkages]
        if not isinstance(nclasses, list):
            nclasses = [nclasses]
        names = [rec.name for rec in self.records]
        for metric in metrics:
            if not metric in self.get_distance_matrices():
                trees = [rec.tree for rec in self.records]  # preserve ordering
                self.clustering.put_distance_matrix(trees, metric)
            for linkage in linkages:
                for n in nclasses:
                    self.clustering.put_partition(metric, linkage, n,
                            names, criterion=criterion)
                    key = (metric, linkage, n)

                    # self.clustering.concatenate_records(key,
                            # self.records)

    def get_partitions(self):
        return self.clustering.partitions

    def put_clusters(self):
        for key in self.clustering.partitions:
            if key not in self.clustering.clusters:
                self.clustering.concatenate_records(key, self.records)

    def get_clusters(self):
        return self.clustering.clusters

    def get_cluster_records(self):
        """
        Returns all concatenated records from cluster analysis
        """

        rec_list = []
        clusters_dict = self.get_clusters()  # This is the outer dictionary

        for compound_key in clusters_dict:
            result = clusters_dict[compound_key]  # Result object holds info
            for i in range(result.length):  # Iterate over clusters in Result object
                rec_list.append(result.retrieve_concat(i))  # Retrieve the concat

        return rec_list

    def put_cluster_trees(self, program='treecollection', tmpdir='/tmp'
                          ):
        if program not in ['treecollection', 'raxml', 'phyml']:
            print 'unrecognised program {0}'.format(program)
            return
        rec_list = self.get_cluster_records()
        self.put_trees(rec_list=rec_list, program=program,
                       tmpdir=tmpdir)
        self.update_results()

    def update_results(self):
        for result in self.get_clusters().values():
            result.update_score()

    def put_cluster_trees_parallel(self, program='treecollection',
                                   tmpdir='/tmp'):
        if program not in ['treecollection', 'raxml', 'phyml']:
            print 'unrecognised program {0}'.format(program)
            return
        rec_list = self.get_cluster_records()
        if program == 'treecollection':
            cluster_trees_dict = \
                self._TC_parallel_call(rec_list=rec_list, tmpdir=tmpdir)
        elif program == 'raxml':
            cluster_trees_dict = \
                self._raxml_parallel_call(rec_list=rec_list,
                    tmpdir=tmpdir)
        elif program == 'phyml':
            cluster_trees_dict = \
                self._phyml_parallel_call(rec_list=rec_list,
                    tmpdir=tmpdir)
        for rec in rec_list:
            rec.tree = cluster_trees_dict[rec.name]
        self.update_results()

    def get_cluster_trees(self):
        rec_list = sorted(self.get_cluster_records(), key=lambda rec: \
                          rec.name)
        trees = [rec.tree for rec in rec_list]
        return trees

    def get_randomised_alignments(self):

        def pivot(list):
            length = len(list[0])
            new_list = []
            for i in range(length):
                new_list.append(''.join([x[i] for x in list]))
            return new_list

        lengths = [rec.seqlength for rec in self.records]
        datatype = self.records[0].datatype
        concat = copy.deepcopy(self.records[0])
        for rec in self.records[1:]:
            concat += rec
        columns = pivot(concat.sequences)
        shuffle(columns)
        newcols = []
        for l in lengths:
            newcols.append(columns[:l])
            columns = columns[l:]
        newrecs = []
        for col in newcols:
            newseqs = pivot(col)
            newrec = TCSeqRec(headers=concat.headers,
                              sequences=newseqs, datatype=datatype)
            newrecs.append(newrec)
        for i in range(self.length):
            newrecs[i].name = self.records[i].name
        return newrecs

    def make_randomised_copy(self, program='treecollection',
                             tmpdir='/tmp'):
        """
        Lesson learned: don't forget to call pool.close and pool.join
        when multithreading
        """

        other = copy.deepcopy(self)
        other.records = self.get_randomised_alignments()
        other.put_dv_matrices_parallel()
        other.put_trees_parallel(program=program, tmpdir=tmpdir)
        if self.get_clusters():
            other.clustering.clusters = {}
            other.put_clusters()
        if self.get_cluster_trees()[0].newick:
            cluster_program = \
                self.get_cluster_trees()[0].program.lower()
            other.put_cluster_trees_parallel(program=cluster_program,
                    tmpdir=tmpdir)
        return other

    def show_memberships(self):

        partitions = self.get_partitions()
        for compound_key in partitions:
            print ' '.join(str(x) for x in compound_key)
            partition = partitions[compound_key]
            print partition
            print self.clustering.get_memberships(partition)

    def plot_dendrogram(
        self,
        metric,
        link,
        nclasses,
        show=True,
        ):

        plot_object = self.clustering.plot_dendrogram((metric, link,
                nclasses))
        if show:
            plot_object.show()
        return plot_object

    def scree_plot(
        self,
        metric,
        link,
        *args
        ):

        cl = self.get_clusters()
        (x, y) = ([], [])
        for k in sorted(cl):
            if metric in k and link in k:
                y.append(cl[k].score)
                x.append(k[-1])
        plt.plot(x, y, *args)

    def __init__(
        self,
        input_dir=None,
        records=None,
        file_format='fasta',
        datatype='protein',
        helper='./class_files/DV_wrapper.drw',
        tmpdir='/tmp',
        ):

        self.dir = input_dir
        self.files = None
        self.records = records
        self.clustering = Clustering()
        self.length = 0

        if input_dir:
            files = self.get_files(input_dir, file_format)
            self.put_records(files, file_format, datatype)
            self.sanitise_records()
            self.put_dv_matrices_parallel(helper=helper, tmpdir=tmpdir)
        elif records:

            self.sanitise_records()
            self.put_dv_matrices_parallel(helper=helper, tmpdir=tmpdir)

        self.length = len(self.records)

    def __str__(self):
        s = 'SequenceCollection object:\n'
        s += 'Contains {0} alignments\n'.format(self.length)
        s += 'Source: {0}\n'.format(self.dir)
        return s
